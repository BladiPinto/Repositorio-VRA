@article{,
   abstract = {The substation automation system consists of many different complex assets and data flows. The system is also often externally connected to allow for remote management. The complexity and remote access to the substation automation system makes it vulnerable to cyber attacks. It also makes it difficult to assess the overall security of the system. One method of assessing the potential threats against a system is threat modeling. In this paper we create a language for producing threat models specifically for the substation automation systems. We focus on the method used to create the language where we review industry designs, build the language based on existing languages and consider attack scenarios from a literature study. Finally we present the language, model two different attack scenarios and generate attack graphs from the threat models.},
   author = {Engla Rencelj Ling and Mathias Ekstedt},
   doi = {10.1016/J.IJCIP.2023.100601},
   issn = {1874-5482},
   journal = {International Journal of Critical Infrastructure Protection},
   keywords = {Attack graph,Cyber security,Substation automation systems,Threat modeling language,Vulnerability analysis},
   month = {7},
   pages = {100601},
   publisher = {Elsevier},
   title = {A threat modeling language for generating attack graphs of substation automation systems},
   volume = {41},
   year = {2023},
}
@article{Junaid2022,
   abstract = {Efficient and effective methods are required to construct a model to rapidly extractdifferent sentiments from large volumes of text. To augment the performance of the models, contemporary developments in Natural Language Processing (NLP) have been utilized by researchers to work on several model architecture and pretraining tasks. This work explores several models based on transformer architecture and analyses its performance. In this work, the researchersusea dataset to answer the question of whether or not transformers work significantly well for figurative language and not just literal language classification. The results of various models are compared and have come up as a result of research over time. The studyexplains why it is necessary for computers to understand the occurrence of figurative language, why it is yet a challenge and is being intensively worked on to date, and how it is different from literal language classification. This research also covers how well these models train on a specific type of figurative language and generalize on a few other similar types.},
   author = {Taha Junaid and D. Sumathi and A. N. Sasikumar and S. Suthir and J. Manikandan and Rashmita Khilar and P. G. Kuppusamy and M. Janardhana Raju},
   doi = {10.1016/J.COMPELECENG.2022.108051},
   issn = {0045-7906},
   journal = {Computers and Electrical Engineering},
   keywords = {Architecture,Figurative language,Fine tuning,Hyperbole,Long Short Term Memory (LSTM),Natural language processing,Rhetorical questions,Sarcasm,Sentiment analysis,Transformers},
   month = {7},
   pages = {108051},
   publisher = {Pergamon},
   title = {A comparative analysis of transformer based models for figurative language classification},
   volume = {101},
   year = {2022},
}
@article{Savci2023,
   abstract = {Since Turkish is an agglutinative language and contains reduplication, idiom, and metaphor words, Turkish texts are sources of information with extremely rich meanings. For this reason, the processing and classification of Turkish texts according to their characteristics is both time-consuming and difficult. In this study, the performances of pre-trained language models for multi-text classification using Autotrain were compared in a 250 K Turkish dataset that we created. The results showed that the BERTurk (uncased, 128 k) language model on the dataset showed higher accuracy performance with a training time of 66 min compared to the other models and the CO2 emission was quite low. The ConvBERTurk mC4 (uncased) model is also the best-performing second language model. As a result of this study, we have provided a deeper understanding of the capabilities of pre-trained language models for Turkish on machine learning.},
   author = {Pinar Savci and Bihter Das},
   doi = {10.1016/J.HELIYON.2023.E15670},
   issn = {2405-8440},
   issue = {5},
   journal = {Heliyon},
   keywords = {Artificial intelligence,AutoNLP,Autotrain,Multi-text classification,Natural language processing,Pre-trained language models},
   month = {5},
   pages = {e15670},
   publisher = {Elsevier},
   title = {Comparison of pre-trained language models in terms of carbon emissions, time and accuracy in multi-label text classification using AutoML},
   volume = {9},
   year = {2023},
}
@article{Chandran2023,
   abstract = {Topic models are unsupervised machine learning techniques that output clusters of “topics” represented as co-occurring words with their associated probability distributions. Topic modeling algorithms find latent themes from large document collections by understanding their context. On the other hand, string kernels are supervised machine-learning techniques that quantify string similarities without explicit string encoding. We propose TopicStriKer, a model combining the advantages of unsupervised topic modeling with supervised string kernels for text classification tasks. The co-occurring topic words per topic and topic proportions per document obtained are used to reduce the document corpus to a topic-word sequence. This reduced representation is then used for text classification with the aid of string kernels, significantly improving accuracy and reducing training time. Experiments on the bag-of-words kernel-based string embeddings using the proposed algorithm outperform the traditional text classification approaches. This work extensively compares string kernels with topic modeling on various performance metrics to establish our findings.},
   author = {Nikhil V. Chandran and V. S. Anoop and S. Asharaf},
   doi = {10.1016/J.RINENG.2023.100949},
   issn = {2590-1230},
   journal = {Results in Engineering},
   keywords = {String embedding,String kernels,Text classification,Topic modeling,Topic sequence},
   month = {3},
   pages = {100949},
   publisher = {Elsevier},
   title = {TopicStriKer: A topic kernels-powered approach for text classification},
   volume = {17},
   year = {2023},
}
@article{Wei2023,
   abstract = {Fault text classification is a prerequisite task for railway engineers based historical train operation data to diagnose vehicle on-board equipment (VOBE) faults and formulate maintenance strategies. Aiming at the low efficiency and accuracy of manual fault text classification, based on Bidirectional Gated Recurrent Unit (BiGRU) and improved attention mechanism (IAtt), an intelligent VOBE fault text classification method is proposed in this paper. Combining the characteristics of the VOBE faults text, also called application event log (AElog) files, the Labeled-Doc2vec is used to generate sentence embedding to realize the vectorized representation of the fault texts, then input sentence embedding into BiGRU to extract the fault text features as the improved attention mechanism layer. Finally, the high-dimensional fault text features outputted by hidden are input into Softmax to complete the fault text classification. The experimental results show that the proposed method can analyze the semantics of fault text according to the train running state before and after the fault time, that is, it can realize text classification by combining context. Compared with other methods, the method in this paper obtains the optimal accuracy, precision, recall and F1-score, which shows that the proposed method can be applied to fault text classification of VOBE, effectively reduces the labor cost of fault text classification in practice, and improves the efficiency of fault text classification of VOBE.},
   author = {Wei Wei and Xiaoqiang Zhao},
   doi = {10.1016/J.JRTPM.2023.100372},
   issn = {2210-9706},
   journal = {Journal of Rail Transport Planning & Management},
   keywords = {Attentional mechanism,Fault text classification,Gated recurrent unit,Sentence embedding,Vehicle on-board equipment},
   month = {6},
   pages = {100372},
   publisher = {Elsevier},
   title = {Fault text classification of on-board equipment in high-speed railway based on labeled-Doc2vec and BiGRU},
   volume = {26},
   year = {2023},
}
@article{Dillion2023,
   abstract = {Recent work suggests that language models such as GPT can make human-like judgments across a number of domains. We explore whether and when language models might replace human participants in psychological science. We review nascent research, provide a theoretical model, and outline caveats of using AI as a participant.},
   author = {Danica Dillion and Niket Tandon and Yuling Gu and Kurt Gray},
   doi = {10.1016/J.TICS.2023.04.008},
   issn = {1364-6613},
   journal = {Trends in Cognitive Sciences},
   keywords = {artificial intelligence,judgments,language models,morality,participants,research methods},
   month = {5},
   pmid = {37173156},
   publisher = {Elsevier Current Trends},
   title = {Can AI language models replace human participants?},
   year = {2023},
}
